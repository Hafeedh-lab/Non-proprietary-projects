{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h39FscdPXE8ouM2FzdkgHcz6AM2IQRxf",
      "authorship_tag": "ABX9TyNuv33mxJv0rgu+qupXMXmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hafeedh-lab/Non-proprietary-projects/blob/main/Updated%20updated%20DOCX_TO_MD.ipynb%20lol\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTu7TClib4_"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  OOGA-BOOGA DOCXâ†’MARKDOWN BLAST-OFF for Colab ğŸ¦„ğŸ’¥\n",
        "#  â€¢ BOOM! Turn every .docx into .md like a mad scientist ğŸ¤“ğŸ§ª\n",
        "#  â€¢ Error-splatters, mega-thread juggling & ninja file-checks ğŸš€ğŸ‘¾\n",
        "#  â€¢ Pandoc under the hood doing the heavy lifting ğŸ› ï¸ğŸ˜\n",
        "#  â€¢ Field-tested Aug 2025 on Colabâ€™s Python 3.10 rave party ğŸ‰ğŸ\n",
        "# ============================================================\n",
        "#Install pypandoc you dummy\n",
        "!pip install pypandoc\n",
        "import os\n",
        "import glob\n",
        "import pypandoc\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "from typing import List, Tuple, Optional\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "# ğŸš¨ DEPENDENCY SUMMONING RITUAL: pypandoc, python-docx & tqdm, engage! âœ¨\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q \"pypandoc>=1.12\" python-docx tqdm\n",
        "\n",
        "# ğŸŒ® PANDOC DELIVERY TRUCK: apt-get update + pandoc fiesta incoming! ğŸŠ\n",
        "print(\"Installing pandoc...\")\n",
        "!apt-get update && apt-get install -y pandoc\n",
        "\n",
        "# â”€â”€ 2. DRIVE MOUNT MAYHEM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive like a pro wrestler slams a table...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# â”€â”€ 3. LOGGING CHAOS FACTORY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/conversion_log.txt'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# â”€â”€ 4. CONFIG CIRCUS RINGMASTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class Config:\n",
        "    # ğŸ  Edit these because your Drive is YOUR kingdom\n",
        "    INPUT_FOLDER  = '/content/drive/MyDrive/DOCX TO MD TEST'  # â† where your .docx beasts roam\n",
        "    OUTPUT_FOLDER = '/content/drive/MyDrive/DOCX TO MD TEST'  # â† where the .md loot will hide\n",
        "\n",
        "    # âš¡ï¸ PERFORMANCE JUICE SETTINGS\n",
        "    MAX_WORKERS   = 4    # how many worker bees you unleashing ğŸğŸğŸğŸ\n",
        "    SKIP_EXISTING = True # skip the snooze-fest if .md is already chillin'\n",
        "    VALIDATE_DOCX = True # sniff out fake .docx like a truffle hog ğŸ–\n",
        "\n",
        "    # ğŸª„ PANDOC SPELLBOOK\n",
        "    PANDOC_FORMAT = 'gfm'                        # GitHub-flavoured markdown = yum\n",
        "    PANDOC_ARGS   = ['--wrap=none', '--extract-media=./media']  # no wraps + media loot\n",
        "\n",
        "def validate_docx_file(file_path: str) -> bool:\n",
        "    \"\"\"ğŸ‘€ â€œIs this .docx real or a ghost?â€ Returns True if legit.\"\"\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        Document(file_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"OOPSIE invalid .docx at {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_file_hash(file_path: str) -> str:\n",
        "    \"\"\"ğŸ” MD5 SMASHER: Create a fingerprint so we know if file changed.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                hash_md5.update(chunk)\n",
        "        return hash_md5.hexdigest()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def should_convert_file(docx_path: str, md_path: str, skip_existing: bool) -> bool:\n",
        "    \"\"\"ğŸ§ â€œConvert or nah?â€ Checks if .md is old or missing.\"\"\"\n",
        "    if not skip_existing:\n",
        "        return True\n",
        "    if not os.path.exists(md_path):\n",
        "        return True\n",
        "\n",
        "    # â²ï¸ Time-travel test: only redo if source is FRESHER than target\n",
        "    docx_mtime = os.path.getmtime(docx_path)\n",
        "    md_mtime   = os.path.getmtime(md_path)\n",
        "    return docx_mtime > md_mtime\n",
        "\n",
        "def convert_single_file(args: Tuple[str, str, dict]) -> Tuple[bool, str, Optional[str]]:\n",
        "    \"\"\"\n",
        "    ğŸ”„ Converts one .docx â†’ .md\n",
        "    Returns (didItWork, filePath, errorMsg)\n",
        "    \"\"\"\n",
        "    docx_path, output_folder, config = args\n",
        "\n",
        "    try:\n",
        "        # ğŸ•µï¸â€â™€ï¸ Optional docx detective: legit file?\n",
        "        if config.get('validate_docx', True) and not validate_docx_file(docx_path):\n",
        "            return False, docx_path, \"INVALID .docx, BRO!\"\n",
        "\n",
        "        # ğŸ—ºï¸ Build the output path, keep those folders intact!\n",
        "        rel_path = os.path.relpath(docx_path, config['input_folder'])\n",
        "        basename = os.path.splitext(os.path.basename(rel_path))[0] + '.md'\n",
        "\n",
        "        # ğŸ—ï¸ Make subfolders like a boss\n",
        "        subdir = os.path.dirname(rel_path)\n",
        "        if subdir:\n",
        "            output_subdir = os.path.join(output_folder, subdir)\n",
        "            os.makedirs(output_subdir, exist_ok=True)\n",
        "            md_path = os.path.join(output_subdir, basename)\n",
        "        else:\n",
        "            md_path = os.path.join(output_folder, basename)\n",
        "\n",
        "        # â­ï¸ SKIP-ZONE: If it's already done, bail out\n",
        "        if not should_convert_file(docx_path, md_path, config.get('skip_existing', True)):\n",
        "            return True, docx_path, \"SKIPPEDâ€”ITâ€™S ALREADY FRESH AF\"\n",
        "\n",
        "        # ğŸ¤– PANDOC TRANSFORMER ACTIVATED: Let the magic happen!\n",
        "        pypandoc.convert_file(\n",
        "            source_file=docx_path,\n",
        "            to=config.get('pandoc_format', 'gfm'),\n",
        "            outputfile=md_path,\n",
        "            extra_args=config.get('pandoc_args', ['--wrap=none'])\n",
        "        )\n",
        "        return True, docx_path, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, docx_path, str(e)\n",
        "\n",
        "def find_docx_files(input_folder: str) -> List[str]:\n",
        "    \"\"\"ğŸ” Go spelunking in the folder & find all .docx treasures.\"\"\"\n",
        "    docx_files = []\n",
        "    input_path = Path(input_folder)\n",
        "    if not input_path.exists():\n",
        "        raise FileNotFoundError(f\"Whoa, folder not found: {input_folder}\")\n",
        "\n",
        "    # ğŸ§­ Recursively search, but dodge temp ghost files (starting with ~$)\n",
        "    for docx_file in input_path.rglob('*.docx'):\n",
        "        if not docx_file.name.startswith('~$'):\n",
        "            docx_files.append(str(docx_file))\n",
        "\n",
        "    return sorted(docx_files)\n",
        "\n",
        "def main():\n",
        "    \"\"\"ğŸ‰ MAIN EVENT: Kick off the wild conversion shindig!\"\"\"\n",
        "    config = {\n",
        "        'input_folder':  Config.INPUT_FOLDER,\n",
        "        'output_folder': Config.OUTPUT_FOLDER,\n",
        "        'max_workers':   Config.MAX_WORKERS,\n",
        "        'skip_existing': Config.SKIP_EXISTING,\n",
        "        'validate_docx': Config.VALIDATE_DOCX,\n",
        "        'pandoc_format': Config.PANDOC_FORMAT,\n",
        "        'pandoc_args':   Config.PANDOC_ARGS\n",
        "    }\n",
        "\n",
        "    # ğŸš¦ START YOUR ENGINES at THIS VERY SECOND!\n",
        "    print(f\"Starting conversion at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}â€”LETâ€™S GOOO!\")\n",
        "    logger.info(f\"Input folder: {config['input_folder']}\")\n",
        "    logger.info(f\"Output folder: {config['output_folder']}\")\n",
        "\n",
        "    # ğŸ“‚ Make output folder if itâ€™s missing, no biggie\n",
        "    os.makedirs(config['output_folder'], exist_ok=True)\n",
        "\n",
        "    # ğŸ” FILE HUNT START: search for .docx critters\n",
        "    print(\"Looking for .docx files...\")\n",
        "    try:\n",
        "        docx_files = find_docx_files(config['input_folder'])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERROR ALERT: {e}\")\n",
        "        return\n",
        "\n",
        "    if not docx_files:\n",
        "        print(\"No .docx files hereâ€”aborting mission, bye!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(docx_files)} .docx filesâ€”LETâ€™S ROLL OUT!\")\n",
        "\n",
        "    # ğŸ“ Prep the to-do list for each file\n",
        "    conversion_args = [(docx_path, config['output_folder'], config) for docx_path in docx_files]\n",
        "\n",
        "    # ğŸ­ THREAD POOL CARNIVAL & PROGRESS BAR COASTER\n",
        "    successful_conversions = 0\n",
        "    failed_conversions     = 0\n",
        "    skipped_conversions    = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config['max_workers']) as executor:\n",
        "        future_to_file = {\n",
        "            executor.submit(convert_single_file, args): args[0]\n",
        "            for args in conversion_args\n",
        "        }\n",
        "\n",
        "        with tqdm(total=len(docx_files), desc='Converting files... WOOHOO') as pbar:\n",
        "            for future in as_completed(future_to_file):\n",
        "                success, file_path, error_msg = future.result()\n",
        "                rel_path = os.path.relpath(file_path, config['input_folder'])\n",
        "\n",
        "                if success:\n",
        "                    if error_msg and \"SKIPPED\" in error_msg:\n",
        "                        skipped_conversions += 1\n",
        "                        logger.info(f\"â­ï¸  SKIPPED: {rel_path}\")\n",
        "                    else:\n",
        "                        successful_conversions += 1\n",
        "                        logger.info(f\"âœ… NAILED IT: {rel_path}\")\n",
        "                else:\n",
        "                    failed_conversions += 1\n",
        "                    logger.error(f\"âŒ EPIC FAIL: {rel_path} - {error_msg}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    # ğŸ“Š GRAND FINALE SUMMARY\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ‡ Conversion Report ğŸ‡\")\n",
        "    print(f\"  âœ… Successes: {successful_conversions}\")\n",
        "    print(f\"  â­ï¸ Skips:      {skipped_conversions}\")\n",
        "    print(f\"  âŒ Failures:   {failed_conversions}\")\n",
        "    print(f\"  ğŸ“ Output:     {config['output_folder']}\")\n",
        "    print(f\"  ğŸ“ Log:        /content/conversion_log.txt\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if failed_conversions > 0:\n",
        "        print(\"âš ï¸ WHOOPSâ€”some files bombed. Check that log, champ!\")\n",
        "\n",
        "# â”€â”€ 5. LAUNCH CONVERSION BLAST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "import os\n",
        "\n",
        "# ğŸ—ºï¸ Map rootess: twekha deez paths so notbuk vibez match ur Driv hangoutz ğŸ¤™ OOGA-BOOGA!\n",
        "original_notebook = \"/content/drive/MyDrive/Colab Notebooks/DOCX TO MD.ipynb\"\n",
        "cleaned_notebook  = \"cleaned_notebook.ipynb\"\n",
        "output_dir        = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "output_path       = os.path.join(output_dir, cleaned_notebook)\n",
        "\n",
        "if not os.path.exists(original_notebook):\n",
        "    print(f\"Error: Original notebook not found at {original_notebook}\")  # ğŸ˜± BROOO, notbuk gwn missin lik a sock in naikdr?\n",
        "else:\n",
        "    try:\n",
        "        print(f\"Loading notebook from: {original_notebook}\")  # ğŸš€ Blstff readin dat notbuk bruh!\n",
        "        nb = nbformat.read(original_notebook, as_version=4)\n",
        "\n",
        "        # 1ï¸âƒ£ SMMASH MOD: yeet da buggee 'wigdgets' metadata frum da notbuk rohwt ğŸ˜¤ğŸ”¨ YEAAAHHHH!\n",
        "        if \"widgets\" in nb.metadata:\n",
        "            print(\"Removing 'widgets' from notebook metadataâ€¦\")  # ğŸ§¹ Sweepin out dem wigdet krumz\n",
        "            nb.metadata.pop(\"widgets\")\n",
        "\n",
        "        # 2ï¸âƒ£ NINJAA SWEEEP: huntz down 'wigdgets' in each sell & ouputâ€”no hidding allouddd ğŸ‘€ğŸš€ AHHHHH!\n",
        "        for idx, cell in enumerate(nb.cells):\n",
        "            if \"widgets\" in cell.metadata:\n",
        "                print(f\"  â€¢ OOGA-BOOGA! Strippin 'wigdgets' frum sell {idx}\")  # ğŸ’¥ Cell {idx} cleenzed n glanzed!\n",
        "                cell.metadata.pop(\"widgets\")\n",
        "            # ğŸ‘» Sometimez wigdetz lurk in outpootzâ€”ghozt-bust em!\n",
        "            for out_idx, output in enumerate(cell.get(\"outputs\", [])):\n",
        "                md = output.get(\"metadata\", {})\n",
        "                if \"widgets\" in md:\n",
        "                    print(f\"    â€“ POOF! Nixxin 'wigdgets' frum oput {out_idx} of sell {idx}\")  # âœ¨ Byeee-byee wigdet bling bling\n",
        "                    md.pop(\"widgets\")\n",
        "\n",
        "        # 3ï¸âƒ£ VICTORYY SAAAVE: drop da squeeky-clean notbuk bak to Driv ğŸŒŸğŸ“˜ YAHHHHHH!\n",
        "        nbformat.write(nb, output_path)\n",
        "        print(f\"Cleaned notebook saved as: {output_path}\")  # ğŸ‰ Notbuk glow-up komplete!\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")  # ğŸ’¥ OOPSIE ZOMGBBQ: somethin went kabooom: {e}\n"
      ],
      "metadata": {
        "id": "hZaQUNtPtBsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}