{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h39FscdPXE8ouM2FzdkgHcz6AM2IQRxf",
      "authorship_tag": "ABX9TyNuv33mxJv0rgu+qupXMXmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hafeedh-lab/Non-proprietary-projects/blob/main/Updated%20updated%20DOCX_TO_MD.ipynb%20lol\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTu7TClib4_"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#  OOGA-BOOGA DOCX‚ÜíMARKDOWN BLAST-OFF for Colab ü¶Ñüí•\n",
        "#  ‚Ä¢ BOOM! Turn every .docx into .md like a mad scientist ü§ìüß™\n",
        "#  ‚Ä¢ Error-splatters, mega-thread juggling & ninja file-checks üöÄüëæ\n",
        "#  ‚Ä¢ Pandoc under the hood doing the heavy lifting üõ†Ô∏èüòé\n",
        "#  ‚Ä¢ Field-tested Aug 2025 on Colab‚Äôs Python 3.10 rave party üéâüêç\n",
        "# ============================================================\n",
        "#Install pypandoc you dummy\n",
        "!pip install pypandoc\n",
        "import os\n",
        "import glob\n",
        "import pypandoc\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "from typing import List, Tuple, Optional\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "# üö® DEPENDENCY SUMMONING RITUAL: pypandoc, python-docx & tqdm, engage! ‚ú®\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q \"pypandoc>=1.12\" python-docx tqdm\n",
        "\n",
        "# üåÆ PANDOC DELIVERY TRUCK: apt-get update + pandoc fiesta incoming! üéä\n",
        "print(\"Installing pandoc...\")\n",
        "!apt-get update && apt-get install -y pandoc\n",
        "\n",
        "# ‚îÄ‚îÄ 2. DRIVE MOUNT MAYHEM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive like a pro wrestler slams a table...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ‚îÄ‚îÄ 3. LOGGING CHAOS FACTORY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('/content/conversion_log.txt'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ‚îÄ‚îÄ 4. CONFIG CIRCUS RINGMASTER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "class Config:\n",
        "    # üè† Edit these because your Drive is YOUR kingdom\n",
        "    INPUT_FOLDER  = '/content/drive/MyDrive/DOCX TO MD TEST'  # ‚Üê where your .docx beasts roam\n",
        "    OUTPUT_FOLDER = '/content/drive/MyDrive/DOCX TO MD TEST'  # ‚Üê where the .md loot will hide\n",
        "\n",
        "    # ‚ö°Ô∏è PERFORMANCE JUICE SETTINGS\n",
        "    MAX_WORKERS   = 4    # how many worker bees you unleashing üêùüêùüêùüêù\n",
        "    SKIP_EXISTING = True # skip the snooze-fest if .md is already chillin'\n",
        "    VALIDATE_DOCX = True # sniff out fake .docx like a truffle hog üêñ\n",
        "\n",
        "    # ü™Ñ PANDOC SPELLBOOK\n",
        "    PANDOC_FORMAT = 'gfm'                        # GitHub-flavoured markdown = yum\n",
        "    PANDOC_ARGS   = ['--wrap=none', '--extract-media=./media']  # no wraps + media loot\n",
        "\n",
        "def validate_docx_file(file_path: str) -> bool:\n",
        "    \"\"\"üëÄ ‚ÄúIs this .docx real or a ghost?‚Äù Returns True if legit.\"\"\"\n",
        "    try:\n",
        "        from docx import Document\n",
        "        Document(file_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"OOPSIE invalid .docx at {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_file_hash(file_path: str) -> str:\n",
        "    \"\"\"üîê MD5 SMASHER: Create a fingerprint so we know if file changed.\"\"\"\n",
        "    hash_md5 = hashlib.md5()\n",
        "    try:\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                hash_md5.update(chunk)\n",
        "        return hash_md5.hexdigest()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def should_convert_file(docx_path: str, md_path: str, skip_existing: bool) -> bool:\n",
        "    \"\"\"üßê ‚ÄúConvert or nah?‚Äù Checks if .md is old or missing.\"\"\"\n",
        "    if not skip_existing:\n",
        "        return True\n",
        "    if not os.path.exists(md_path):\n",
        "        return True\n",
        "\n",
        "    # ‚è≤Ô∏è Time-travel test: only redo if source is FRESHER than target\n",
        "    docx_mtime = os.path.getmtime(docx_path)\n",
        "    md_mtime   = os.path.getmtime(md_path)\n",
        "    return docx_mtime > md_mtime\n",
        "\n",
        "def convert_single_file(args: Tuple[str, str, dict]) -> Tuple[bool, str, Optional[str]]:\n",
        "    \"\"\"\n",
        "    üîÑ Converts one .docx ‚Üí .md\n",
        "    Returns (didItWork, filePath, errorMsg)\n",
        "    \"\"\"\n",
        "    docx_path, output_folder, config = args\n",
        "\n",
        "    try:\n",
        "        # üïµÔ∏è‚Äç‚ôÄÔ∏è Optional docx detective: legit file?\n",
        "        if config.get('validate_docx', True) and not validate_docx_file(docx_path):\n",
        "            return False, docx_path, \"INVALID .docx, BRO!\"\n",
        "\n",
        "        # üó∫Ô∏è Build the output path, keep those folders intact!\n",
        "        rel_path = os.path.relpath(docx_path, config['input_folder'])\n",
        "        basename = os.path.splitext(os.path.basename(rel_path))[0] + '.md'\n",
        "\n",
        "        # üèóÔ∏è Make subfolders like a boss\n",
        "        subdir = os.path.dirname(rel_path)\n",
        "        if subdir:\n",
        "            output_subdir = os.path.join(output_folder, subdir)\n",
        "            os.makedirs(output_subdir, exist_ok=True)\n",
        "            md_path = os.path.join(output_subdir, basename)\n",
        "        else:\n",
        "            md_path = os.path.join(output_folder, basename)\n",
        "\n",
        "        # ‚è≠Ô∏è SKIP-ZONE: If it's already done, bail out\n",
        "        if not should_convert_file(docx_path, md_path, config.get('skip_existing', True)):\n",
        "            return True, docx_path, \"SKIPPED‚ÄîIT‚ÄôS ALREADY FRESH AF\"\n",
        "\n",
        "        # ü§ñ PANDOC TRANSFORMER ACTIVATED: Let the magic happen!\n",
        "        pypandoc.convert_file(\n",
        "            source_file=docx_path,\n",
        "            to=config.get('pandoc_format', 'gfm'),\n",
        "            outputfile=md_path,\n",
        "            extra_args=config.get('pandoc_args', ['--wrap=none'])\n",
        "        )\n",
        "        return True, docx_path, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, docx_path, str(e)\n",
        "\n",
        "def find_docx_files(input_folder: str) -> List[str]:\n",
        "    \"\"\"üîç Go spelunking in the folder & find all .docx treasures.\"\"\"\n",
        "    docx_files = []\n",
        "    input_path = Path(input_folder)\n",
        "    if not input_path.exists():\n",
        "        raise FileNotFoundError(f\"Whoa, folder not found: {input_folder}\")\n",
        "\n",
        "    # üß≠ Recursively search, but dodge temp ghost files (starting with ~$)\n",
        "    for docx_file in input_path.rglob('*.docx'):\n",
        "        if not docx_file.name.startswith('~$'):\n",
        "            docx_files.append(str(docx_file))\n",
        "\n",
        "    return sorted(docx_files)\n",
        "\n",
        "def main():\n",
        "    \"\"\"üéâ MAIN EVENT: Kick off the wild conversion shindig!\"\"\"\n",
        "    config = {\n",
        "        'input_folder':  Config.INPUT_FOLDER,\n",
        "        'output_folder': Config.OUTPUT_FOLDER,\n",
        "        'max_workers':   Config.MAX_WORKERS,\n",
        "        'skip_existing': Config.SKIP_EXISTING,\n",
        "        'validate_docx': Config.VALIDATE_DOCX,\n",
        "        'pandoc_format': Config.PANDOC_FORMAT,\n",
        "        'pandoc_args':   Config.PANDOC_ARGS\n",
        "    }\n",
        "\n",
        "    # üö¶ START YOUR ENGINES at THIS VERY SECOND!\n",
        "    print(f\"Starting conversion at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}‚ÄîLET‚ÄôS GOOO!\")\n",
        "    logger.info(f\"Input folder: {config['input_folder']}\")\n",
        "    logger.info(f\"Output folder: {config['output_folder']}\")\n",
        "\n",
        "    # üìÇ Make output folder if it‚Äôs missing, no biggie\n",
        "    os.makedirs(config['output_folder'], exist_ok=True)\n",
        "\n",
        "    # üîé FILE HUNT START: search for .docx critters\n",
        "    print(\"Looking for .docx files...\")\n",
        "    try:\n",
        "        docx_files = find_docx_files(config['input_folder'])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"ERROR ALERT: {e}\")\n",
        "        return\n",
        "\n",
        "    if not docx_files:\n",
        "        print(\"No .docx files here‚Äîaborting mission, bye!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(docx_files)} .docx files‚ÄîLET‚ÄôS ROLL OUT!\")\n",
        "\n",
        "    # üìù Prep the to-do list for each file\n",
        "    conversion_args = [(docx_path, config['output_folder'], config) for docx_path in docx_files]\n",
        "\n",
        "    # üé≠ THREAD POOL CARNIVAL & PROGRESS BAR COASTER\n",
        "    successful_conversions = 0\n",
        "    failed_conversions     = 0\n",
        "    skipped_conversions    = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config['max_workers']) as executor:\n",
        "        future_to_file = {\n",
        "            executor.submit(convert_single_file, args): args[0]\n",
        "            for args in conversion_args\n",
        "        }\n",
        "\n",
        "        with tqdm(total=len(docx_files), desc='Converting files... WOOHOO') as pbar:\n",
        "            for future in as_completed(future_to_file):\n",
        "                success, file_path, error_msg = future.result()\n",
        "                rel_path = os.path.relpath(file_path, config['input_folder'])\n",
        "\n",
        "                if success:\n",
        "                    if error_msg and \"SKIPPED\" in error_msg:\n",
        "                        skipped_conversions += 1\n",
        "                        logger.info(f\"‚è≠Ô∏è  SKIPPED: {rel_path}\")\n",
        "                    else:\n",
        "                        successful_conversions += 1\n",
        "                        logger.info(f\"‚úÖ NAILED IT: {rel_path}\")\n",
        "                else:\n",
        "                    failed_conversions += 1\n",
        "                    logger.error(f\"‚ùå EPIC FAIL: {rel_path} - {error_msg}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    # üìä GRAND FINALE SUMMARY\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéá Conversion Report üéá\")\n",
        "    print(f\"  ‚úÖ Successes: {successful_conversions}\")\n",
        "    print(f\"  ‚è≠Ô∏è Skips:      {skipped_conversions}\")\n",
        "    print(f\"  ‚ùå Failures:   {failed_conversions}\")\n",
        "    print(f\"  üìÅ Output:     {config['output_folder']}\")\n",
        "    print(f\"  üìù Log:        /content/conversion_log.txt\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if failed_conversions > 0:\n",
        "        print(\"‚ö†Ô∏è WHOOPS‚Äîsome files bombed. Check that log, champ!\")\n",
        "\n",
        "# ‚îÄ‚îÄ 5. LAUNCH CONVERSION BLAST ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "import os\n",
        "\n",
        "# üó∫Ô∏è Map rootess: twekha deez paths so notbuk vibez match ur Driv hangoutz ü§ô OOGA-BOOGA!\n",
        "original_notebook = \"/content/drive/MyDrive/Colab Notebooks/DOCX TO MD.ipynb\"\n",
        "cleaned_notebook  = \"cleaned_notebook.ipynb\"\n",
        "output_dir        = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "output_path       = os.path.join(output_dir, cleaned_notebook)\n",
        "\n",
        "if not os.path.exists(original_notebook):\n",
        "    print(f\"Error: Original notebook not found at {original_notebook}\")  # üò± BROOO, notbuk gwn missin lik a sock in naikdr?\n",
        "else:\n",
        "    try:\n",
        "        print(f\"Loading notebook from: {original_notebook}\")  # üöÄ Blstff readin dat notbuk bruh!\n",
        "        nb = nbformat.read(original_notebook, as_version=4)\n",
        "\n",
        "        # 1Ô∏è‚É£ SMMASH MOD: yeet da buggee 'wigdgets' metadata frum da notbuk rohwt üò§üî® YEAAAHHHH!\n",
        "        if \"widgets\" in nb.metadata:\n",
        "            print(\"Removing 'widgets' from notebook metadata‚Ä¶\")  # üßπ Sweepin out dem wigdet krumz\n",
        "            nb.metadata.pop(\"widgets\")\n",
        "\n",
        "        # 2Ô∏è‚É£ NINJAA SWEEEP: huntz down 'wigdgets' in each sell & ouput‚Äîno hidding allouddd üëÄüöÄ AHHHHH!\n",
        "        for idx, cell in enumerate(nb.cells):\n",
        "            if \"widgets\" in cell.metadata:\n",
        "                print(f\"  ‚Ä¢ OOGA-BOOGA! Strippin 'wigdgets' frum sell {idx}\")  # üí• Cell {idx} cleenzed n glanzed!\n",
        "                cell.metadata.pop(\"widgets\")\n",
        "            # üëª Sometimez wigdetz lurk in outpootz‚Äîghozt-bust em!\n",
        "            for out_idx, output in enumerate(cell.get(\"outputs\", [])):\n",
        "                md = output.get(\"metadata\", {})\n",
        "                if \"widgets\" in md:\n",
        "                    print(f\"    ‚Äì POOF! Nixxin 'wigdgets' frum oput {out_idx} of sell {idx}\")  # ‚ú® Byeee-byee wigdet bling bling\n",
        "                    md.pop(\"widgets\")\n",
        "\n",
        "        # 3Ô∏è‚É£ VICTORYY SAAAVE: drop da squeeky-clean notbuk bak to Driv üåüüìò YAHHHHHH!\n",
        "        nbformat.write(nb, output_path)\n",
        "        print(f\"Cleaned notebook saved as: {output_path}\")  # üéâ Notbuk glow-up komplete!\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")  # üí• OOPSIE ZOMGBBQ: somethin went kabooom: {e}\n"
      ],
      "metadata": {
        "id": "hZaQUNtPtBsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}