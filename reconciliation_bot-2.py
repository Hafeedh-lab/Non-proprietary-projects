# -*- coding: utf-8 -*-
"""RECONCILIATION BOT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nO30OlCVdo8CqmcYY19PTaahED46PyAZ
"""

!pip install selenium beautifulsoup4 gspread oauth2client python-dateutil
import requests
import bs4
import html
import time
import json
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from datetime import datetime
import re
from dateutil import parser as dtparse

# Constants
USER = "zulikhabinvestment@yahoo.com"
PASS = "Zulikah1964"
LOGIN_URL = "https://lawma.trackitpay.com/sys/signin"
SPREADSHEET_KEY = '1s3iWQWXjfKGdEsZ-req8-cUSnA9upqyBas3zm9FluHI'
CREDENTIALS_FILE = '/content/drive/MyDrive/Colab Notebooks/SUPPORT FILES /my_friend.json'

def norm_amount(s):
    # keep digits and decimal point only
    return re.sub(r'[^\d.]', '', s)

def norm_date(s):
    # try to parse whatever we received and return ISO YYYY-MM-DD
    try:
        return dtparse.parse(s, dayfirst=False, yearfirst=True).date().isoformat()
    except (ValueError, TypeError):
        return s.strip()        # fallback to raw string

def make_key(cust, amt, dt):
    return f"{cust.strip().lower()}-{norm_amount(amt)}-{norm_date(dt)}"

# Helper to strip tags & decode entities
def clean(cell):
    if not isinstance(cell, str):
        cell = str(cell)
    text = bs4.BeautifulSoup(cell, "lxml").get_text(" ", strip=True)
    return html.unescape(text)  # turns &#x20A6; into â‚¦

# Set up and perform login with Selenium
def perform_login():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=opts)
    wait = WebDriverWait(driver, 15)
    try:
        # Navigate to login page
        driver.get(LOGIN_URL)
        # Fill in credentials and submit
        email_input = wait.until(
            EC.presence_of_element_located((By.ID, "userName"))
        )
        email_input.send_keys(USER)
        driver.find_element(By.ID, "password").send_keys(PASS)
        driver.find_element(By.CSS_SELECTOR, "button[type='submit']").click()
        # Wait for successful login
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.side-nav")))
        print("Login successful!")
        print(f"Current URL: {driver.current_url}")
        print(f"Page title: {driver.title}")

        #Get cookies
        cookies = driver.get_cookies()
        return cookies, driver

    except Exception as e:
        print(f"Login failed: {e}")
        if driver:
            driver.quit()
        return None, None

# Function to fetch transaction data directly from the browser
def fetch_transaction_data(driver):
    wait = WebDriverWait(driver, 15)
    driver.get("https://lawma.trackitpay.com/transaction")
    wait.until(EC.presence_of_element_located(
        (By.CSS_SELECTOR, "table.dataTable")))

    # â”€â”€â”€  Try the dropdown first â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    length_changed = False
    target_len = "500"  # or "All"
    try:
        sel = wait.until(EC.element_to_be_clickable(
            (By.CSS_SELECTOR, "select[name$='_length']")))
        original_value = sel.get_attribute("value")
        if original_value != target_len:
            Select(sel).select_by_visible_text(target_len)
            length_changed = True
    except Exception as e:
        # Dropdown missing or unclickable â€“ fall back to JS
        print("ðŸ›ˆ Couldn't click length <select>: {}".format(e))
        try:
            driver.execute_script("""
                try {
                    var dt = $($.fn.dataTable.tables()[0]).DataTable();
                    dt.page.len(%s).draw();
                } catch(e) {}
            """ % (-1 if target_len.lower()=="all" else target_len))
            length_changed = True
        except Exception as js_e:
            print("ðŸ›ˆ JavaScript fallback also failed:", js_e)

    # Wait for row count to grow
    start_time = time.time()
    old_count  = len(driver.find_elements(
        By.CSS_SELECTOR, "table.dataTable tbody tr"))
    while length_changed and time.time() - start_time < 15:
        curr_count = len(driver.find_elements(
            By.CSS_SELECTOR, "table.dataTable tbody tr"))
        if curr_count > old_count:
            break
        time.sleep(0.5)

    #Scrape rows
    rows = []
    for tr in driver.find_elements(By.CSS_SELECTOR,
                                   "table.dataTable tbody tr"):
        tds = [td.text.strip() for td in tr.find_elements(By.CSS_SELECTOR, "td")]
        if len(tds) >= 10:
            customer = tds[2]
            amount   = tds[6]
            raw_date = tds[9]

            # Normalise to "17 May 2025"
            try:
                created = dtparse.parse(raw_date, fuzzy=True).strftime("%-d %b %Y")
            except Exception:
                created = raw_date.replace("st","").replace("nd","") \
                                   .replace("rd","").replace("th","")

            rows.append([customer, amount, created])

    print(f"âœ… Browser scrape got {len(rows):,} rows")
    return rows

# Google Sheets Integration
def setup_gspread():
    """Setup and authenticate with Google Sheets API"""
    # Define the scopes
    scopes = [
        'https://spreadsheets.google.com/feeds',
        'https://www.googleapis.com/auth/drive'
    ]

    # Load credentials from JSON file
    creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_FILE, scopes)

    # Authorize gspread client
    client = gspread.authorize(creds)

    # Open the spreadsheet
    sheet = client.open_by_key(SPREADSHEET_KEY).sheet1
    return sheet

def refresh_google_sheet(ws, data_rows, header_rows: int = 0):
    """
    Replace every old transaction in the sheet with the freshly-scraped list.

    Parameters
    ----------
    ws : gspread.Worksheet
        The Google-sheet worksheet object you want to refresh.
    data_rows : list[list[str]]
        Cleaned rows from TrackItPay, e.g. [['Customer', '1000', '2025-05-17'], ...]
    header_rows : int, optional
        Number of header rows that must be preserved at the top of the sheet.
    """
    try:
        # 1ï¸âƒ£  Sort by *real* date (oldest â†’ newest)
        data_rows_sorted = sorted(
            data_rows,
            key=lambda r: dtparse.parse(r[2], dayfirst=True),
            reverse=True            # flip to True if you prefer newest-first
        )

        # 2ï¸âƒ£  Grab any existing headers so we can put them back later
        headers = []
        if header_rows > 0:
            try:
                headers = ws.get_all_values()[:header_rows]
            except Exception as e:
                print(f"Warning: could not retrieve headers: {e}")
                headers = [["Customer", "Amount", "Date"]][:header_rows]

        # 3ï¸âƒ£  Clear the sheet and re-populate it
        ws.clear()

        if headers:
            ws.append_rows(headers)

        if data_rows_sorted:
            ws.append_rows(data_rows_sorted)

        print(f"âœ… Sheet refreshed with {len(data_rows_sorted):,} transactions.")

    except Exception as e:
        import traceback
        print(f"âŒ Error updating Google Sheet: {e}")
        print(traceback.format_exc())


# Main execution
def main():
    print("Starting LAWMA TrackitPay transaction scraper...")
    print("=" * 50)

    # Get current time
    start_time = datetime.now()
    print(f"Process started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # Login and get cookies
    cookies, driver = perform_login()
    if driver:  # Changed condition from checking cookies to checking driver
        try:
            # Fetch and process data
            data_rows = fetch_transaction_data(driver)  # Removed cookies parameter

            # Print sample of the data (if available)
            if data_rows and len(data_rows) > 0:
                print("\nSample data (first 3 rows):")
                for row in data_rows[:min(3, len(data_rows))]:
                    print(row)

                # Setup Google Sheets and update with the scraped data
                worksheet = setup_gspread()  # Get the worksheet
                refresh_google_sheet(worksheet, data_rows)
            else:
                print("No data retrieved. Check the logs above for error details.")
        finally:
            if driver:
                driver.quit()
    else:
        print("Failed to login. Cannot proceed.")

    # Get end time and calculate duration
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"Process completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total duration: {duration.total_seconds():.2f} seconds")
    print("=" * 50)

if __name__ == "__main__":
    main()